I wrote a simple C program that adds two integers and prints the result. 
The program was compiled using **RISC-V GCC** with two optimization levels: `-O1` and `-Ofast`. I used a **Makefile** to automate the process and generated object dumps with `riscv64-unknown-elf-objdump` to compare the machine code.
 `-O1` applies basic optimizations, while `-Ofast` uses aggressive optimizations for better performance.
 Finally, I used **SPIKE** to simulate the program's execution. 
This process allowed me to analyze the impact of different optimization levels on the compiled code.
